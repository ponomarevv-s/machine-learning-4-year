{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "feature_selection.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.9"
    },
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7rcrcty_U3qy"
      },
      "source": [
        "# Методы отбора признаков\n",
        "\n",
        "Отбор признаков – это процесс выбора признаков, имеющих наиболее тесные взаимосвязи с целевой переменной.\n",
        "\n",
        "Присутствие в данных неинформативных признаков приводит к снижению точности многих моделей, особенно линейных, таких как линейная и логистическая регрессия.\n",
        "\n",
        "Отбор признаков перед моделированием обеспечивает три следующих преимущества:\n",
        "\n",
        "* Уменьшение переобучения. Чем меньше избыточных данных, тем меньше возможностей для модели принимать решения на основе «шума».\n",
        "* Повышение точности. Чем меньше противоречивых данных, тем выше точность.\n",
        "* Сокращение времени обучения. Чем меньше данных, тем быстрее обучается модель.\n",
        "\n",
        "Подробное руководство по отбору признаков с помощью scikit-learn вы можете найти в документации к этой библиотеке в разделе [Feature selection](http://scikit-learn.org/stable/modules/feature_selection.html).\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NNFVQrj7a9nu"
      },
      "source": [
        "В наших примерах мы будем работать с набором данных, содержащим информацию о случаях сахарного диабета среди индейцев Пима: [Pima Indians diabetes](https://gist.githubusercontent.com/chaityacshah/899a95deaf8b1930003ae93944fd17d7/raw/3d35de839da708595a444187e9f13237b51a2cbe/pima-indians-diabetes.csv). Все признаки являются числовыми, а задача представляет собой бинарную классификацию.\n",
        "\n",
        "Ниже рассмотрены 4 метода для отбора признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LHhV4DKLbP1G"
      },
      "source": [
        "## 1. Одномерный отбор признаков\n",
        "\n",
        "Признаки, имеющие наиболее выраженную взаимосвязь с целевой переменной, могут быть отобраны с помощью статистических критериев. Библиотека scikit-learn содержит класс [SelectKBest](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.SelectKBest.html#sklearn.feature_selection.SelectKBest), реализующий одномерный отбор признаков (univariate feature selection). Этот класс можно применять совместно с различными статистическими критериями для отбора заданного количества признаков.\n",
        "\n",
        "В примере ниже используется критерий хи-квадрат (chi-squared test) для неотрицательных признаков, чтобы отобрать 4 лучших признака."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "glASjO5gcC_0",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "02ba5d34-00a0-443f-add5-65d18f5edb73"
      },
      "source": [
        "# Feature Extraction with Univariate Statistical Tests (Chi-squared for classification)\n",
        "\n",
        "import pandas\n",
        "import numpy\n",
        "from sklearn.feature_selection import SelectKBest\n",
        "from sklearn.feature_selection import chi2\n",
        "\n",
        "# load data\n",
        "url = \"https://gist.githubusercontent.com/chaityacshah/899a95deaf8b1930003ae93944fd17d7/raw/3d35de839da708595a444187e9f13237b51a2cbe/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, skiprows=1, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# feature extraction\n",
        "test = SelectKBest(score_func=chi2, k=4)\n",
        "fit = test.fit(X, Y)\n",
        "\n",
        "# summarize scores\n",
        "numpy.set_printoptions(precision=3)\n",
        "print(fit.scores_)\n",
        "features = fit.transform(X)\n",
        "\n",
        "# summarize selected features\n",
        "print(features[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[ 111.52  1411.887   17.605   53.108 2175.565  127.669    5.393  181.304]\n",
            "[[148.    0.   33.6  50. ]\n",
            " [ 85.    0.   26.6  31. ]\n",
            " [183.    0.   23.3  32. ]\n",
            " [ 89.   94.   28.1  21. ]\n",
            " [137.  168.   43.1  33. ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3PBbeoOMcJQu"
      },
      "source": [
        "Мы видим оценки для каждого признака и 4 отобранных признака (с наивысшими оценками): plas, test, mass и age."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "t-MZAJ2ubgxf"
      },
      "source": [
        "## 2. Рекурсивное исключение признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "54rGXvzCdJ_V"
      },
      "source": [
        "Метод рекурсивного исключения признаков (recursive feature elimination, RFE) реализует следующий алгоритм: модель обучается на исходном наборе признаков и оценивает их значимость, затем исключается один или несколько наименее значимых признаков, модель обучается на оставшихся признаках, и так далее, пока не останется заданное количество лучших признаков. В документации scikit-learn вы можете подробнее прочитать о классе [RFE](http://scikit-learn.org/stable/modules/generated/sklearn.feature_selection.RFE.html#sklearn.feature_selection.RFE).\n",
        "\n",
        "В примере ниже метод RFE применяется в сочетании с логистической регрессией для отбора 3-х лучших признаков. Для совместного использования с RFE можно выбирать различные модели, важно лишь, чтобы они были достаточно эффективны и совместимы с RFE."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Vi3huDgcCak",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "9af1c0d3-b66d-4e72-a4ae-b7a606042a03"
      },
      "source": [
        "# Feature Extraction with RFE\n",
        "from pandas import read_csv\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# load data\n",
        "url = \"https://gist.githubusercontent.com/chaityacshah/899a95deaf8b1930003ae93944fd17d7/raw/3d35de839da708595a444187e9f13237b51a2cbe/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, skiprows=1, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# feature extraction\n",
        "model = LogisticRegression()\n",
        "rfe = RFE(model, 3)\n",
        "fit = rfe.fit(X, Y)\n",
        "print(\"Num Features: %d\") % fit.n_features_\n",
        "print(\"Selected Features: %s\") % fit.support_\n",
        "print(\"Feature Ranking: %s\") % fit.ranking_"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Num Features: 3\n",
            "Selected Features: [ True False False False False  True  True False]\n",
            "Feature Ranking: [1 2 3 5 6 1 1 4]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/linear_model/logistic.py:433: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
            "  FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Yuy2sR48cNiD"
      },
      "source": [
        "Мы видим, что в результате были отобраны 3 лучших признака: preg, mass, pedi. Отобранные признаки помечены значением «True» в массиве support_ и значением «1» в массиве ranking_."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eYvceqyybma8"
      },
      "source": [
        "## 3. Метод главных компонент\n",
        "\n",
        "Метод главных компонент (principal component analysis, PCA) позволяет уменьшить размерность данных с помощью преобразования на основе линейной алгебры. Пользователь может задать требуемое количество измерений (главных компонент) в результирующих данных.\n",
        "\n",
        "В примере ниже мы выделяем 3 главных компоненты с помощью PCA.\n",
        "\n",
        "Подробная информация о классе [PCA](http://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html) доступна в документации scikit-learn. Если вас заинтересовала математика PCA, обратитесь к статье в [Википедии](https://en.wikipedia.org/wiki/Principal_component_analysis)."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RIEDv8fTcRAX",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        },
        "outputId": "a51ccee6-38f8-451c-82bd-7869a793334d"
      },
      "source": [
        "# Feature Extraction with PCA\n",
        "import numpy\n",
        "from pandas import read_csv\n",
        "from sklearn.decomposition import PCA\n",
        "\n",
        "# load data\n",
        "url = \"https://gist.githubusercontent.com/chaityacshah/899a95deaf8b1930003ae93944fd17d7/raw/3d35de839da708595a444187e9f13237b51a2cbe/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, skiprows=1, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# feature extraction\n",
        "pca = PCA(n_components=3)\n",
        "fit = pca.fit(X)\n",
        "features = fit.transform(X)\n",
        "\n",
        "# summarize components\n",
        "print(\"Explained Variance: %s\") % fit.explained_variance_ratio_\n",
        "print(features[0:5,:])"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Explained Variance: [0.889 0.062 0.026]\n",
            "[[-7.571e+01 -3.595e+01 -7.261e+00]\n",
            " [-8.236e+01  2.891e+01 -5.497e+00]\n",
            " [-7.463e+01 -6.791e+01  1.946e+01]\n",
            " [ 1.108e+01  3.490e+01 -5.302e-02]\n",
            " [ 8.974e+01 -2.747e+00  2.521e+01]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x9WsPmodcZlA"
      },
      "source": [
        "Как видим, результат преобразования (3 главных компоненты) совсем не похож на исходные данные."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YaynFkApbqQk"
      },
      "source": [
        "## 4. Отбор на основе важности признаков"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DLJGjHelcg0M"
      },
      "source": [
        "Ансамблевые алгоритмы на основе деревьев решений, такие как случайный лес (random forest), позволяют оценить важность признаков.\n",
        "\n",
        "В представленном ниже примере мы обучаем классификатор ExtraTreesClassifier, чтобы с его помощью определить важность признаков. Подробнее о классе [ExtraTreesClassifier](http://scikit-learn.org/stable/modules/generated/sklearn.ensemble.ExtraTreesClassifier.html) можно узнать из документации scikit-learn."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "l4ZPY4racemT",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 89
        },
        "outputId": "e80f1585-3117-4d20-c12a-d3078a68552e"
      },
      "source": [
        "# Feature Importance with Extra Trees Classifier\n",
        "\n",
        "from pandas import read_csv\n",
        "from sklearn.ensemble import ExtraTreesClassifier \n",
        "\n",
        "# load data\n",
        "url = \"https://gist.githubusercontent.com/chaityacshah/899a95deaf8b1930003ae93944fd17d7/raw/3d35de839da708595a444187e9f13237b51a2cbe/pima-indians-diabetes.csv\"\n",
        "names = ['preg', 'plas', 'pres', 'skin', 'test', 'mass', 'pedi', 'age', 'class']\n",
        "dataframe = pandas.read_csv(url, skiprows=1, names=names)\n",
        "array = dataframe.values\n",
        "X = array[:,0:8]\n",
        "Y = array[:,8]\n",
        "\n",
        "# feature extraction\n",
        "model = ExtraTreesClassifier()\n",
        "model.fit(X, Y)\n",
        "print(model.feature_importances_)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[0.106 0.253 0.094 0.089 0.068 0.135 0.115 0.14 ]\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python2.7/dist-packages/sklearn/ensemble/forest.py:246: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "crRXqKoncoJT"
      },
      "source": [
        "Мы получили оценки для каждого признака. Чем больше значение оценки, тем важнее признак. Таким образом, согласно данному методу отбора, тремя наиболее важными признаками являются: plas, age и mass."
      ]
    }
  ]
}